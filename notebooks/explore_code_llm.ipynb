{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/RAG_secure_code_generation/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain import HuggingFaceHub, PromptTemplate, LLMChain\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig,CodeGenTokenizer,CodeGenConfig, CodeGenForCausalLM,CodeLlamaTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values\n",
    "dotenv_values = dotenv_values()\n",
    "hf_key = dotenv_values['HUGGINGFACEHUB_API_TOKEN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/RAG_secure_code_generation/.venv/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFaceHub(repo_id = 'Salesforce/codegen-6B-mono', \n",
    "                     model_kwargs = {\n",
    "                         \"temperature\" : 1,\n",
    "                         #\"max_length\" : 500,\n",
    "                     })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = '''def sum_two_numbers(num1 : int, num2 : int)->int:\n",
    "    \"\"\"Given two numbers, return the sum of them.\"\"\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def sum_two_numbers(num1 : int, num2 : int)->int:\\n    \"\"\"Given two numbers, return the sum of them.\"\"\"\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    return num1+num2\\n\\ndef add_numbers'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(template, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 1.05k/1.05k [00:00<00:00, 4.53MB/s]\n",
      "pytorch_model.bin.index.json: 100%|██████████| 36.3k/36.3k [00:00<00:00, 38.2MB/s]\n",
      "pytorch_model-00001-of-00007.bin: 100%|██████████| 9.90G/9.90G [01:28<00:00, 112MB/s]\n",
      "pytorch_model-00002-of-00007.bin: 100%|██████████| 9.86G/9.86G [01:31<00:00, 108MB/s]\n",
      "pytorch_model-00003-of-00007.bin: 100%|██████████| 9.85G/9.85G [01:32<00:00, 106MB/s]\n",
      "pytorch_model-00004-of-00007.bin: 100%|██████████| 9.86G/9.86G [06:21<00:00, 25.8MB/s]\n",
      "pytorch_model-00005-of-00007.bin: 100%|██████████| 9.85G/9.85G [01:35<00:00, 104MB/s]\n",
      "pytorch_model-00006-of-00007.bin: 100%|██████████| 9.86G/9.86G [01:35<00:00, 103MB/s] \n",
      "pytorch_model-00007-of-00007.bin: 100%|██████████| 4.08G/4.08G [00:39<00:00, 104MB/s] \n",
      "Downloading shards: 100%|██████████| 7/7 [14:47<00:00, 126.75s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:52<00:00,  7.53s/it]\n",
      "/workspaces/RAG_secure_code_generation/.venv/lib/python3.10/site-packages/transformers/utils/hub.py:374: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "generation_config.json: 100%|██████████| 111/111 [00:00<00:00, 393kB/s]\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"bigcode/CodeLlama-34b-Python-hf\"\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#configuration = CodeGenConfig(token = hf_key, trust_remote_code = True)\n",
    "\n",
    "# Initializing a model (with random weights) from the configuration\n",
    "#model = CodeGenForCausalLM(configuration).to(device)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint,trust_remote_code=True, load_in_4bit = True,token = hf_key, device_map = \"auto\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 677/677 [00:00<00:00, 2.70MB/s]\n",
      "vocab.json: 100%|██████████| 777k/777k [00:00<00:00, 18.2MB/s]\n",
      "merges.txt: 100%|██████████| 442k/442k [00:00<00:00, 72.8MB/s]\n",
      "tokenizer.json: 100%|██████████| 2.06M/2.06M [00:00<00:00, 33.8MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 532/532 [00:00<00:00, 4.27MB/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, trust_remote_code = True, token = hf_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''def detect_xss(http_get_request: str)->bool: \n",
    "\"\"\" Check if in the given http_get_request there is an XSS exploit, considering also the possible evansions that an attacker can perform.\"\"\" \n",
    "'''\n",
    "\n",
    "# payload = http_get_request.split('?')[1]\n",
    "# parameters = list(payload.split('&'))\n",
    "# couples_dict = dict(map(lambda x: x.split('='), parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def detect_xss(http_get_request: str)->bool: \\n\"\"\" Check if in the given http_get_request there is an XSS exploit, considering also the possible evansions that an attacker can perform.\"\"\" \\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.encode(text, return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "completion = model.generate(inputs, \n",
    "                            max_new_tokens = 500,\n",
    "                    do_sample =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def detect_xss(http_get_request: str)->bool: \n",
      "\"\"\" Check if in the given http_get_request there is an XSS exploit, considering also the possible evansions that an attacker can perform.\"\"\" \n",
      "\n",
      "import re\n",
      "\n",
      "def detect_xss(http_get_request):\n",
      "  detected=False\n",
      "  # Check GET request URL\n",
      "  for p in xssParams:\n",
      "    for pp in p['checkparams']:\n",
      "      if re.search(pp[\"test\"],http_get_request,re.IGNORECASE):\n",
      "        detected=True\n",
      "        if \"exploit\" in pp:\n",
      "          for e in pp[\"exploit\"]:\n",
      "            http_get_request=http_get_request.replace(e[\"content\"],e[\"replace\"])\n",
      "        #print(\"[!] XSS Detected in parameter \\\"\"+pp[\"name\"]+\"\\\": \"+str(pp))\n",
      "        xssFound.append({\"request\":http_get_request,\"xssParams\":pp,\"detected\":detected})\n",
      "        print(\"[!] XSS Detected: Request to %s with parameter \\\"%s\\\" -> %s\"%(p['url'],pp[\"name\"],pp[\"value\"]))\n",
      "\n",
      "\"\"\"\n",
      "xssParams.append({\n",
      "  'url':'/xyz/example.php?param1=x&param2=y', \n",
      "  'checkParams':[{'name':'param1','value':'x','check':'xssattack'}, \n",
      "  {'name':'param1','value':'y','check':'html'},\n",
      "  {'name':'param3','value':'z','check':'xssattack','exploit': [\n",
      "  { 'content':'z','replace':'z1234' }\n",
      "]}]\n",
      "})\n",
      "\"\"\"\n",
      "<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "#,truncate_before_pattern=[r\"\\n\\n^#\", \"^'''\", \"\\n\\n\\n\"]\n",
    "output = tokenizer.decode(completion[0])\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50256"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4299,  2160,    62, 11545,    62,    77, 17024,     7, 22510,    16,\n",
       "          1058,   493,    11,   997,    17,  1058,   493,     8,  3784,   600,\n",
       "            25,   198, 50284, 37811, 15056,   734,  3146,    11,  1441,   262,\n",
       "          2160,   286,   606,   526, 15931,   198, 50284,  7783,   997,    16,\n",
       "          1343,   997,    17,   198,   198,  4299, 29162,    62, 11545,    62,\n",
       "            77, 17024,     7, 22510,    16,  1058,   493,    11,   997,    17,\n",
       "          1058,   493,     8,  3784,   600,    25,   198, 50284, 37811, 15056,\n",
       "           734,  3146,    11,  1441,   262,  1720,   286,   606,   526, 15931,\n",
       "           198, 50284,  7783,   997,    16,  1635,   997,    17,   198,   198,\n",
       "          4299, 14083,    62, 11545,    62,    77, 17024,     7, 22510,    16,\n",
       "          1058,   493,    11,   997,    17,  1058,   493,     8,  3784,   600,\n",
       "            25,   198, 50284, 37811, 15056,   734,  3146,    11,  1441,   262,\n",
       "         23611,  1153,   286,   606,   526, 15931,   198, 50284,  7783,   997,\n",
       "            16,  1220,   997,    17,   198,   198,  4299, 34128,    62, 11545,\n",
       "            62,    77, 17024,     7, 22510,    16,  1058,   493,    11,   997,\n",
       "            17,  1058,   493,     8,  3784,   600,    25,   198, 50284, 37811,\n",
       "         15056,   734,  3146,    11,  1441,   262,  3580,   286,   606,   526,\n",
       "         15931,   198, 50284,  7783,   997,    16,   532,   997,    17,   198,\n",
       "           198,  4299,   751,    62, 11545,    62,    77, 17024,     7, 22510,\n",
       "            16,  1058,   493,    11,   997,    17,  1058,   493,     8,  3784,\n",
       "           600,    25,   198, 50284, 37811, 15056,   734,  3146,    11,  1441,\n",
       "           262,  3090,   286,   606,   526, 15931,   198, 50284,  7783,   997,\n",
       "            16,  1343,   997,    17,   198,   198,  4299, 34128,    62, 11545,\n",
       "            62,    77, 17024,     7, 22510,    16]], device='cuda:0')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<|endoftext|>',\n",
       " 'eos_token': '<|endoftext|>',\n",
       " 'unk_token': '<|endoftext|>',\n",
       " 'additional_special_tokens': ['<|endoftext|>',\n",
       "  '<fim-prefix>',\n",
       "  '<fim-middle>',\n",
       "  '<fim-suffix>',\n",
       "  '<fim-pad>']}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"<|endoftext|>\" in output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
