# Artifacts of 'Evaluating and Improving Security-Critical Code Generated by LLMs'

All the artifacts are available at [FigShare](https://figshare.com/s/bb6fb28ab79d846daeb3).
In FigShare there is a zip file that contains the ground-truth datasets for XSS Detection and SQLi Detection, the Generated Function Runs, the Synthetic Datasets and the results of the experiments.

The following will analyze the different parts of the artifacts.

## Ground-truth datasets

Inside the zip file there is a folder named *datasets*.
It has a subfolder for every task, and inside those there are 3 csv file: *train.csv*, *val.csv* and *test.csv*.
The represented the processed and splitted version of [FMereani Dataset](https://github.com/fmereani/Cross-Site-Scripting-XSS/blob/master/XSSDataSets/Payloads.csv) and SOFIA Dataset, respectilvely used for XSS Detection and SQLi Detection task.

The csv files are structure with 2 columns. *Payloads* column contains the input string for the Generated Functions, while the column *Class* contains the ground-truth label (Malicious/Benign).


## Generated Function Runs
Inside the zip file there is a folder named *generated_function_runs*.
The tree structure of the folder is quite complicated because of pratical needs, but it is not interesting from an artifact point-of-view.
It is possible to find every single Generated Function Run inside the folder *run_0*.
Inside that, there are 40 subfolders representing every single Generated Function.
Inside every subfolder there is the *.py* file containing the function itself.
Additionally, two JSON files reports the success or failure outcome of the testing, and the performance of the Generated Function on *val.csv* and *test.csv*.

With the 40 subfolders, there are also two JSON files summarizing the results for the entire Generated Function Run, *prompt.txt* that reports the input prompt to the LLM, and *parameters.json* that summarizes the Combination and other metadata regarding the Generated Function Run.

Notice that, using the replication package to generate a new Generated Function Run and to test it with ground-truth or synthetic datasets, the resulting output structure is much heavier.
We reported a selected subset of the files generated in the process for storage reasons, trying to keep the most informative files that, together with the Results artifact, are able to capture all the results reported in the paper.


## Synthetic Datasets
Inside the zip file there is a folder named *synthetic_datasets*.
The tree structure is similar to the one presented for Generated Function Runs.
It is possible to find every single set of 10 Synthetic Datasets generated from a Combination inside the folder *run_0*.
Inside that, 10 csv files are present, representing the outputs generated by the LLMs.
In addition, there is *prompt.txt* that reports the input prompt to the LLM, and *parameters.json* that summarizes the Combination and other metadata regarding the Synthetic Dataset.
Following the same concept explained for Generated Function Runs, we reported a selected subset of the files generated in the process.

## Results
Inside the zip file there is a folder named *results*.
Two subfolders are present: *generated_function_runs* and *self-ranking*
Both of them contain one subfolder for every task and a csv file for every considered metric.

Notice that, we will refer to the metric columns as *m*, but the name will be different inside every specific csv file.
For example, we will explain the meaning of *avg_m_diff*, but this column will be present as *avg_acc_diff* in *acc.csv* and as *avg_f2_diff* in *f2.csv*

Let's analyze first the results file in *generated_function_runs*.
Inside that, every row represent a Model-Temperature pair.
Every column represent the performance obtained on the *test_set* by a specific combination on RAG Usage Parameter and Few-shot examples.
The last column, *avg_m_diff*, gives an overview of the effect of RAG on a specific Model-Temperature pair.
Given the set $N_s$ containing all the possible value of Few-shot examples (in the provided experiments it is $\{0, 2, 6, 10\}$) dvided by 2, since, when we created the experimental framework, we were considering this number as the Few-shot examples per class (in practice you will find the set of numbers $\{0, 1, 3, 5\}$).
*avg_m_diff* is calculated as $\frac{1}{|N_s|} * \sum_{n \in N_s}(M('rag \_ n') - M('no \_ rag \_n'))$

Let's analyze the results file in *self-ranking*.
Every row represent the pair between the Generated Function Run $U$ and the synthetic dataset $S$, represented respectively in the columns *generated_function_run* and *synthetic_dataset*, and reported in the form *(model\_temperature)\_(generation_mode)\_(examples)*. Generation mode represents the used prompt and it is inside the set $\{zero\_shot, few\_shot, rag, rag\_few\_shot\}$
To give some examples and to have consistency with the results reported in the paper, the combination (GPT4T, 0.0, 10, T), is equivalent to *gpt-4-0125-preview_0.0_rag_few_shot_5* in this column notation. 
The first column represent the average performance of a Generated Function Run (already present in the other files, but reported here to have an clear comparison).
For every considered value of $k$, 3 columns are reported:
*top\_k* represent the performance  *top\_k* functions of $U$ selected using $S$ to perform Self-Ranking.
*top\_k\_m\_improvement* represent the improvement obatined with respect to the average performance by selecting the *top\_k* functions of $U$ using $S$ to perform Self-Ranking.
*top\_k\_m\_diff* represents the difference between *top\_k\_m* and the optimal *top\_k* performance selecting the "real" *top\_k* functions using the Test set.
All these metrics are discussed more in depth in the Section IV of the paper.